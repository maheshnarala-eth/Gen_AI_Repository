# ğŸ¤– Gemini RAG Chatbot (Streamlit + LlamaIndex)

A Retrieval-Augmented Generation (RAG) based chatbot built using **Streamlit**, **LlamaIndex**, **Google Gemini LLM**, and **Local HuggingFace Embeddings**.

This project allows you to chat with your own documents (PDF/TXT/etc.) using an AI assistant powered by **Gemini-2.0-Flash**, while avoiding API quota exhaustion issues using local vector embeddings.

---

## âœ… Features

- ğŸ“„ Load and index your own documents from the `Data/` folder  
- ğŸ” Semantic search using vector embeddings  
- ğŸ§  Gemini-powered answer generation  
- âš¡ Local HuggingFace embeddings (no embedding API cost)  
- ğŸš« Prevents 429 quota errors with:
  - Button-based querying
  - Session caching
  - Rate-limit delay
  - Retry with exponential backoff  
- ğŸ–¥ï¸ Interactive chatbot UI built with Streamlit  

---

## ğŸ“‚ Project Structure

QASYSTEM/
â”‚
â”œâ”€â”€ app.py # Main Streamlit chatbot application
â”œâ”€â”€ Data/ # Place your documents here
â”‚ â”œâ”€â”€ file1.pdf
â”‚ â”œâ”€â”€ file2.txt
â”‚
â”œâ”€â”€ .env # Contains your Gemini API Key
â””â”€â”€ README.md # Project documentation


# ğŸ”‘ Setup Gemini API Key

Create a .env file inside the project folder:

GOOGLE_API_KEY=your_actual_gemini_api_key_here


Make sure there are no quotes around the key.


ğŸ“¥ Add Documents

Place your documents inside the Data/ folder:

Supported formats include: 

PDF

TXT

DOCX

Markdown files

Example:

Data/
 â”œâ”€â”€ report.pdf
 â”œâ”€â”€ notes.txt

â–¶ï¸ Run the Chatbot

Start the Streamlit app using:

streamlit run app.py


Once running, open your browser:

http://localhost:8501